{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gmcalixto/llm-military-dictatorship-analysis/blob/main/Inquiring_AI_Models_sensible_topics.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Notebook - análise comparativa de respostas de modelos de LLM a perguntas sobre temas sensíveis"
      ],
      "metadata": {
        "id": "WD-o-Naho36O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Instalando dependências"
      ],
      "metadata": {
        "id": "5VL8vhh0o004"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Instalar as bibliotecas necessárias (se ainda não estiverem instaladas)\n",
        "!pip install transformers sentence-transformers spacy scikit-learn seaborn matplotlib\n",
        "!python -m spacy download pt_core_news_sm"
      ],
      "metadata": {
        "id": "FgKyqrbeo3Tb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Conectando ao Google Drive e gerando diretórios"
      ],
      "metadata": {
        "id": "rPnpKvAlgJLy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Importando Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "zhLDSsEAgNMA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "# Define o diretório base\n",
        "base_directory = \"/content/drive/MyDrive/Universidade/UNEB/Projetos/DataLab/Data_Libray/Biblioteca_projetos/LLM_USP/Entrevistando_AI/\"\n",
        "os.makedirs(base_directory, exist_ok=True)\n",
        "\n",
        "base_directory_csv = os.path.join(base_directory, 'csv') #salvar os logs\n",
        "os.makedirs(base_directory_csv, exist_ok=True)\n",
        "\n",
        "base_directory_viz = os.path.join(base_directory, 'visualizacao') #salvar os csvs\n",
        "os.makedirs(base_directory_viz, exist_ok=True)"
      ],
      "metadata": {
        "id": "oHVfKxy_gN7s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "abrindo base de dados"
      ],
      "metadata": {
        "id": "Mpgd-zKUqO0j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "# Definição do diretório base (já definido anteriormente)\n",
        "file_entrevista = os.path.join(base_directory, \"entrevistando AI.csv\")\n",
        "file_alan = os.path.join(base_directory, \"analise_texto_alan (1).csv\")\n",
        "\n",
        "# Carregar os arquivos como DataFrames\n",
        "df_entrevista = pd.read_csv(file_entrevista)\n",
        "df_alan = pd.read_csv(file_alan)\n",
        "\n",
        "# Exibir as primeiras linhas para verificar o carregamento\n",
        "print(\"df_entrevista:\")\n",
        "display(df_entrevista.head())\n",
        "\n",
        "print(\"\\ndf_alan:\")\n",
        "display(df_alan.head())\n"
      ],
      "metadata": {
        "id": "jy7rsC2SqOP0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "W1aHQKzlPRqA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Abordagem\n",
        "\n",
        "Com base nessa planilha: https://docs.google.com/spreadsheets/d/1gaEFQ3_ZtcLqv8CVQMlm6Q2rEivubRSt/edit?gid=386360956#gid=386360956\n",
        "\n",
        "Foram escolhidos textos que poderiam ser classificados com discurso (1) favorável ao regime militar, (2) desfavorável ao regime militar e (3) neutro ao regime militar.\n",
        "\n",
        "Foram escolhidos 3 textos para análise (destacados em amarelo na planilha):\n",
        "“Assassinato leva estudantes à greve nacional”, presente na linha 4 da planilha, como “desfavorável”\n",
        "“A plenitude democrática”, presente na linha 15, como “favorável”\n",
        "“A história do Ato Institucional n.º 5, presente na linha 19, como “neutro”\n",
        "\n",
        "- Submeti os 3 textos ao GPT-4 (4o) (OpenAI), Sabiá-3 (MaritacaAI) e DeepSeek-R1 com o seguinte prompt: “o texto vindo de jornal brasileiro tem viés favorável ou crítico ao regime militar?”\n",
        "\n",
        "Compilei os resultados na planilha “Resultados das respostas”, indicando os links dos chats gerados no ChatGPT e DeepSeek (menos o Sabiá-3 que não permite gerar link) e reproduzindo as respostas em todos: https://docs.google.com/spreadsheets/d/1XbzPzMsSaQCCCl4AwrpfHaLLe5gxaCE_R-dLjaskfBE/edit?gid=0#gid=0\n",
        "\n",
        "Possíveis pontos de discussão a partir dos resultados:\n",
        "de quais maneiras as respostas dos três modelos coincidem ou divergem da classificação humana? apresentam coerência interna ou respostas contraditórias? quais termos utilizados em cada modelo permitem inferir sutis ou explicitos viéses nas respostas? Há padrões linguísticos ou argumentos recorrentes que revelam um viés implícito?\n",
        "como as diferenças nas bases e treinamento dos modelos parecem influenciar a percepção de viés nos textos?\n",
        "nova formulação do prompt influencia as respostas dos modelos em qual medida? (não testamos)\n",
        "quais termos aparecem com mais frequência em textos classificados como favoráveis, desfavoráveis e neutros? Os modelos são sensíveis a certas palavras ao determinar o viés?\n"
      ],
      "metadata": {
        "id": "c45yVZ03fm2r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Exemplo de uso no Colab ou em um script Python:\n",
        "\n",
        "# 1) Importe ou cole este código em uma célula.\n",
        "# 2) Carregue seu DataFrame (por exemplo, a partir de um CSV):\n",
        "df = df_alan.copy()\n",
        "\n",
        "# 3) Chame a função:\n",
        "result_dict = analyze_model_responses(\n",
        "    df=df,\n",
        "    question_col=\"texto\",\n",
        "    model_cols=['ChatGPT', 'Sabiá-3', 'DeepSeek'],\n",
        "    theme_col=\"posicao\",              # Se não existir a coluna 'tema', passe None\n",
        "    output_dir=base_directory # Diretório de saída\n",
        ")\n",
        "\n",
        "# 4) A função retorna um dicionário com DataFrames, caso queira manipular além do salvamento:\n",
        "#df_sent = result_dict[\"df_sentiment\"]\n"
      ],
      "metadata": {
        "id": "c7USh26_fmpF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Salvar cada DataFrame como variável independente no ambiente Colab\n",
        "df_sentiment = result_dict[\"df_sentiment\"]\n",
        "df_length_classification = result_dict[\"df_length_classification\"]\n",
        "df_similarity_by_question = result_dict[\"df_similarity_by_question\"]\n",
        "df_top_ngrams_by_question_model = result_dict[\"df_top_ngrams_by_question_model\"]\n",
        "df_overall_model_stats = result_dict[\"df_overall_model_stats\"]\n",
        "df_global_similarity = result_dict[\"df_global_similarity\"]\n",
        "\n",
        "# Exibir os DataFrames carregados\n",
        "print(\"DataFrames disponíveis no ambiente:\")\n",
        "for name, df in result_dict.items():\n",
        "    print(f\"- {name} (shape: {df.shape})\")\n",
        "\n",
        "\n",
        "display(df_sentiment.head(5))\n",
        "display(df_length_classification.head(5))\n",
        "display(df_similarity_by_question.head(5))\n",
        "display(df_top_ngrams_by_question_model.head(5))\n",
        "display(df_overall_model_stats.head(5))\n",
        "display(df_global_similarity.head(5))\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "eYze95pt60pL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Descrição do desenho do estudo e metodologia de análise**\n",
        "\n",
        "O estudo busca explorar como diferentes modelos de linguagem posicionam-se sobre temas politicamente sensíveis, com foco na influência de viéses associados ao contexto geopolítico de treinamento e implementação desses modelos. Para isso, foram formuladas perguntas diretas e objetivas sobre escravidão, ditaduras e sistemas de governo no Brasil, China e Estados Unidos. Além disso, uma questão final investigou a posição dos modelos sobre liberdades individuais, de expressão e de manifestação política nesses países. As perguntas foram aplicadas nas interfaces web oficiais dos serviços, considerando versões gratuitas para DeepSeek e Sabiá 3, e a versão padrão do GPT-4o. A metodologia adotou engenharia de prompt para evidenciar possíveis restrições impostas por censura ou filtragem de conteúdo, particularmente no caso da DeepSeek, que demonstrou bloqueios ao tratar de temas como ditaduras na China.\n",
        "\n",
        "As respostas foram analisadas com base em múltiplos critérios, incluindo o tamanho da resposta, vocabulário empregado, similaridade semântica e padrões lexicais. Para isso, foram utilizados modelos de embeddings (*SentenceTransformer*) para aferir proximidade entre respostas e identificar variações na formulação de conteúdos pelos modelos. Foram extraídas *n-grams* e estatísticas de vocabulário, permitindo comparar a riqueza lexical e possíveis diferenças na forma de construção das respostas. A análise de sentimento foi inicialmente considerada, mas posteriormente descartada devido a inconsistências nas classificações entre diferentes modelos (*RoBERTa* e *BERT-multilingual*). Os resultados foram sistematicamente armazenados em um dataframe, permitindo a comparação estruturada entre os modelos e facilitando a visualização de padrões e tendências."
      ],
      "metadata": {
        "id": "mTWg6wGiomF2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Visualização de dados dos vocabulários usados e calculo do C-TF-IDF dos termos"
      ],
      "metadata": {
        "id": "FkwudVoyI-pr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Analise lexical por c-tf-idf para identificação das marcas lexicais de cada modelo (tokens e bigramas)"
      ],
      "metadata": {
        "id": "KYHz8enCYnGv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import ast\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from google.colab import files\n",
        "from IPython.display import display\n",
        "\n",
        "# 1. Upload CSV file\n",
        "uploaded = files.upload()  # Faça o upload do arquivo CSV\n",
        "file_name = list(uploaded.keys())[0]\n",
        "df = pd.read_csv(file_name)\n",
        "\n",
        "# 2. Filtrar os modelos desejados\n",
        "models = [\"ChatGPT\", \"Sabiá-3\", \"DeepSeek\"]\n",
        "df_filtered = df[df[\"modelo\"].isin(models)].copy()\n",
        "\n",
        "# 3. Solicitar ao usuário o nome da coluna contendo o texto/tokens a serem avaliados (default: \"tokens\")\n",
        "column_name = input(\"Enter the column name containing the text/tokens to evaluate (default 'tokens'): \").strip()\n",
        "if column_name == \"\":\n",
        "    column_name = \"tokens\"\n",
        "# Se o usuário inserir mais de um nome (separados por vírgula), usa apenas o primeiro.\n",
        "if ',' in column_name:\n",
        "    column_name = column_name.split(',')[0].strip()\n",
        "\n",
        "# 4. Converter a coluna especificada de string para lista (caso esteja em formato de string)\n",
        "df_filtered[column_name] = df_filtered[column_name].apply(ast.literal_eval)\n",
        "\n",
        "# 5. Agrupar os tokens (ou bigrams) por modelo, tratando cada item da lista como uma unidade inteira\n",
        "def aggregate_texts(series):\n",
        "    # \"series\" contém listas de tokens ou bigrams em cada linha\n",
        "    all_tokens = []\n",
        "    for tokens in series:\n",
        "        # tokens é uma lista; assumimos que cada item já é uma string completa (ex: 'jornal do brasil')\n",
        "        all_tokens.extend(tokens)\n",
        "    # Usa um delimitador pouco provável de ocorrer (\"|||\") para preservar os termos\n",
        "    return \"|||\".join(all_tokens)\n",
        "\n",
        "df_grouped = df_filtered.groupby(\"modelo\")[column_name].apply(aggregate_texts).reset_index()\n",
        "\n",
        "# 6. Solicitar ao usuário as palavras a serem excluídas (ex.: \"regime, texto\")\n",
        "excluded_words_input = input(\"Enter words to exclude (comma separated, e.g., regime, texto). Press ENTER to skip: \")\n",
        "excluded_words = [w.strip().lower() for w in excluded_words_input.split(\",\") if w.strip()]\n",
        "\n",
        "# 7. Solicitar ao usuário o fator de escala para os valores do c-TF-IDF (default: 1)\n",
        "scale_factor_input = input(\"Enter multiplication factor for scaling c-TF-IDF values (default 1): \").strip()\n",
        "try:\n",
        "    scale_factor = float(scale_factor_input) if scale_factor_input else 1.0\n",
        "except:\n",
        "    scale_factor = 1.0\n",
        "\n",
        "# 8. Configurar o CountVectorizer para usar o delimitador \"|||\"\n",
        "#    Ao definir token_pattern=None, o vectorizer usará o tokenizer fornecido.\n",
        "if excluded_words:\n",
        "    vectorizer = CountVectorizer(tokenizer=lambda s: s.split(\"|||\"),\n",
        "                                 token_pattern=None,\n",
        "                                 stop_words=excluded_words)\n",
        "else:\n",
        "    vectorizer = CountVectorizer(tokenizer=lambda s: s.split(\"|||\"),\n",
        "                                 token_pattern=None)\n",
        "\n",
        "# 9. Criar a matriz de contagem usando a coluna agregada\n",
        "count_matrix = vectorizer.fit_transform(df_grouped[column_name])\n",
        "terms = vectorizer.get_feature_names_out()\n",
        "\n",
        "# 10. Calcular c-TF-IDF\n",
        "# 10.1. c-TF: frequência normalizada dos termos por documento (modelo)\n",
        "count_array = count_matrix.toarray()\n",
        "row_sums = count_array.sum(axis=1, keepdims=True)\n",
        "tf = count_array / (row_sums + 1e-9)  # Evita divisão por zero\n",
        "\n",
        "# 10.2. c-IDF: log((N+1)/(df_term+1)) + 1, onde df_term é o número de modelos que contêm o termo\n",
        "df_term = np.count_nonzero(count_array, axis=0)\n",
        "N = len(df_grouped)\n",
        "cidf = np.log((N + 1) / (df_term + 1)) + 1\n",
        "\n",
        "# 10.3. c-TF-IDF: multiplica c-TF por c-IDF e aplica o fator de escala\n",
        "ctfidf = (tf * cidf) * scale_factor\n",
        "\n",
        "# 11. Função para plotar os 10 principais termos de um modelo sem bordas e com rótulos numéricos\n",
        "def plot_ctfidf(model_index, model_name):\n",
        "    row_values = ctfidf[model_index]\n",
        "    top_indices = row_values.argsort()[::-1][:10]\n",
        "    top_terms = [terms[i] for i in top_indices]\n",
        "    top_scores = [row_values[i] for i in top_indices]\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(10, 6))\n",
        "    bars = ax.barh(top_terms, top_scores, color=\"steelblue\", alpha=0.8)\n",
        "    ax.set_xlabel(\"c-TF-IDF Score\")\n",
        "    ax.set_title(f\"Top 10 Characteristic Terms for {model_name}\")\n",
        "    ax.invert_yaxis()  # Maior valor no topo\n",
        "\n",
        "    # Remover todas as bordas (spines)\n",
        "    for spine in ax.spines.values():\n",
        "        spine.set_visible(False)\n",
        "\n",
        "    # Adicionar rótulos numéricos em cada barra\n",
        "    for bar in bars:\n",
        "        width = bar.get_width()\n",
        "        ax.text(width + 0.001, bar.get_y() + bar.get_height()/2,\n",
        "                f'{width:.3f}', va='center', fontsize=10)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# 12. Plotar os gráficos para cada modelo\n",
        "for i, model in enumerate(df_grouped[\"modelo\"]):\n",
        "    plot_ctfidf(i, model)\n",
        "\n",
        "# 13. Criar um DataFrame comparando os 3 principais termos para cada modelo com seus scores e a variância\n",
        "data = []\n",
        "for i, model in enumerate(df_grouped[\"modelo\"]):\n",
        "    row_values = ctfidf[i]\n",
        "    top_indices = row_values.argsort()[::-1][:3]\n",
        "    top_terms = [terms[idx] for idx in top_indices]\n",
        "    top_scores = [row_values[idx] for idx in top_indices]\n",
        "    variance = np.var(top_scores)\n",
        "    data.append({\n",
        "        \"Model\": model,\n",
        "        \"Top Term 1\": top_terms[0],\n",
        "        \"Score 1\": top_scores[0],\n",
        "        \"Top Term 2\": top_terms[1],\n",
        "        \"Score 2\": top_scores[1],\n",
        "        \"Top Term 3\": top_terms[2],\n",
        "        \"Score 3\": top_scores[2],\n",
        "        \"Variance\": variance\n",
        "    })\n",
        "\n",
        "df_top3 = pd.DataFrame(data)\n",
        "print(\"Top 3 Characteristic Terms by Model with c-TF-IDF Score Variance:\")\n",
        "display(df_top3)\n"
      ],
      "metadata": {
        "id": "44LwvvXmSbWi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}